# ai/proxy/main.py
import os
from fastapi import FastAPI, Request, HTTPException
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from pathlib import Path
import uvicorn
import sys
# ==========================================
# 1. è·¯å¾„ä¸ç¯å¢ƒé…ç½® (æœ€å…ˆæ‰§è¡Œ)
# ==========================================

# è·å–å½“å‰æ–‡ä»¶ (main.py) çš„ç»å¯¹è·¯å¾„
# e.g., /home/llt/.../LogSentinel/ai/proxy/main.py
current_file = Path(__file__).resolve()

# è·å–é¡¹ç›®æ ¹ç›®å½• (LogSentinel/)
# main.py -> proxy -> ai -> LogSentinel
project_root = current_file.parent.parent.parent

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ Python æœç´¢è·¯å¾„ï¼Œè§£å†³ "ImportError"
sys.path.append(str(project_root))

# åŠ è½½ .env æ–‡ä»¶
# å‡è®¾ .env åœ¨ ai/ ç›®å½•ä¸‹ (main.py -> proxy -> ai)
dotenv_path = current_file.parent.parent / '.env'
print(f"æ­£åœ¨å°è¯•åŠ è½½ .env æ–‡ä»¶: {dotenv_path}")
load_dotenv(dotenv_path=dotenv_path)

# ==========================================
# 2. å¯¼å…¥æ¨¡å— (å¿…é¡»åœ¨ sys.path è®¾ç½®ä¹‹å)
# ==========================================
# ç°åœ¨å¯ä»¥ä½¿ç”¨ç»å¯¹å¯¼å…¥äº†
from ai.proxy.providers.base import AIProvider
from ai.proxy.providers.gemini import GeminiProvider
from ai.proxy.providers.mock import MockProvider
from ai.proxy.schemas import BatchRequestSchema,ChatRequest,SummarizeRequest,BATCH_PROMPT_TEMPLATE,SUMMARIZE_PROMPT_TEMPLATE


# --- åº”ç”¨è®¾ç½® ---
app = FastAPI(
    title="LogSentinel AI Proxy",
    description="ä¸€ä¸ªç”¨äºä»£ç†ä¸åŒ AI æä¾›å•†æœåŠ¡çš„ä¸­é—´å±‚ã€‚",
    version="1.0.0",
)

# --- Provider å®ä¾‹åŒ–å’Œæ³¨å†Œ ---
# åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºæ‰€æœ‰å¯ç”¨çš„'è½¬æ¢æ’å¤´'å®ä¾‹ï¼Œå¹¶æ”¾å…¥ä¸€ä¸ªå­—å…¸ä¸­è¿›è¡Œç®¡ç†ã€‚
# è¿™ç§æ–¹å¼ä½¿å¾—æ·»åŠ æ–°çš„ Provider å˜å¾—éå¸¸å®¹æ˜“ã€‚
providers: Dict[str, AIProvider] = {}

# å°è¯•åˆå§‹åŒ–å¹¶æ³¨å†Œ Gemini Provider
gemini_api_key = os.getenv("GEMINI_API_KEY")
if gemini_api_key and gemini_api_key != "YOUR_API_KEY":
    try:
        providers["gemini"] = GeminiProvider(api_key=gemini_api_key)
        print("Gemini Provider åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        print(f"åˆå§‹åŒ– Gemini Provider å¤±è´¥: {e}")
else:
    if not gemini_api_key:
        print("ç¯å¢ƒå˜é‡ä¸­æœªæ‰¾åˆ° GEMINI_API_KEYã€‚Gemini Provider å·²ç¦ç”¨ã€‚")
    else:
        print("å‘ç° GEMINI_API_KEY å ä½ç¬¦ã€‚è¯·åœ¨ ai/.env ä¸­æ›¿æ¢ 'YOUR_API_KEY'ã€‚Gemini Provider å·²ç¦ç”¨ã€‚")
providers["mock"] = MockProvider(delay=0.5)
# æœªæ¥å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å¹¶æ³¨å†Œ OpenAI, Claude ç­‰å…¶ä»– Provider
# openai_api_key = os.getenv("OPENAI_API_KEY")
# if openai_api_key:
#     providers["openai"] = OpenAIProvider(api_key=openai_api_key)


# --- API ç«¯ç‚¹å®šä¹‰ ---

@app.get("/")
def read_root():
    return {"status": "LogSentinel AI Proxy æ­£åœ¨è¿è¡Œ", "available_providers": list(providers.keys())}

@app.post("/analyze/{provider_name}")
async def analyze_log(provider_name: str, request: Request):
    """
    å•æ¬¡æ—¥å¿—åˆ†æç«¯ç‚¹ã€‚
    æ¥æ”¶çº¯æ–‡æœ¬æ ¼å¼çš„æ—¥å¿—ã€‚
    """
    provider = providers.get(provider_name)
    if not provider:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æˆ–æœªé…ç½® Provider '{provider_name}'ã€‚")

    try:
        log_text = (await request.body()).decode('utf-8')
        # ä¼˜åŒ–çš„ Promptï¼šæ˜ç¡®åˆ†æè¦æ±‚å’Œé£é™©ç­‰çº§åˆ¤æ–­æ ‡å‡†
        default_prompt = """You are a professional software engineer and log analysis expert. Analyze the following log entry and provide:

1. A concise summary of the error or issue
2. Risk level assessment:
   - 'critical': System crashes, data loss, security vulnerabilities, critical service failures
   - 'error': Performance degradation, non-critical errors, warnings that may escalate
   - 'warning': Informational messages, minor warnings, expected errors
   - 'info': Normal operational logs, state changes, heartbeats
   - 'safe': Verified safe operations
   - 'unknown': Unintelligible logs, binary data, or insufficient context to determine risk
3. Root cause analysis based on the log content
4. Actionable solution or remediation steps

Provide your analysis in a structured format."""
        
        # æ³¨æ„: analyze() ä»…æ¥æ”¶æ¥è‡ª C++ çš„åŸå§‹æ–‡æœ¬ï¼Œå› æ­¤æ— æ³•åœ¨æ­¤æå– api_key/modelã€‚
        # æˆ‘ä»¬ä½¿ç”¨é»˜è®¤çš„ Provider é…ç½®ã€‚
        result = provider.analyze(log_text=log_text, prompt=default_prompt)
        
        return {"provider": provider_name, "analysis": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

@app.post("/analyze/batch/{provider_name}")
async def analyze_log_batch(provider_name: str, request: BatchRequestSchema):
    provider=providers.get(provider_name)
    if not provider:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æˆ–æœªé…ç½® Provider '{provider_name}'ã€‚")
    try:
        logs_list=[item.model_dump() for item in request.batch]

        # ä½¿ç”¨æä¾›çš„ Prompt æˆ–å›é€€åˆ°é»˜è®¤æ¨¡æ¿
        prompt_to_use = request.prompt if request.prompt else BATCH_PROMPT_TEMPLATE

        results=provider.analyze_batch(
            batch_logs=logs_list,
            prompt=prompt_to_use,
            api_key=request.api_key,
            model=request.model
        )
        return {"provider": provider_name, "results": results}
    except Exception as e:
        print(f"[Error] æ‰¹é‡åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/summarize/{provider_name}")
async def summarize_logs(provider_name: str, request_data: SummarizeRequest):
    """
    Reduce é˜¶æ®µï¼šæ¥æ”¶ä¸€æ‰¹ LogAnalysisResultï¼Œç”Ÿæˆå…¨å±€æ€»ç»“ã€‚
    """
    provider = providers.get(provider_name)
    if not provider:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ° Provider '{provider_name}'")
    try:
        # å°† Pydantic å¯¹è±¡åˆ—è¡¨è½¬ä¸º Dict åˆ—è¡¨
        results_list = [item.model_dump() for item in request_data.results]

        # ä½¿ç”¨æä¾›çš„ Prompt æˆ–å›é€€åˆ°é»˜è®¤æ¨¡æ¿
        prompt_to_use = request_data.prompt if request_data.prompt else SUMMARIZE_PROMPT_TEMPLATE

        # è°ƒç”¨ Provider çš„ summarize æ¥å£
        summary_text = provider.summarize(
            summary_logs=results_list,
            prompt=prompt_to_use,
            api_key=request_data.api_key,
            model=request_data.model
        )
        # è¿”å›æ ¼å¼è¦åŒ¹é… C++ ç«¯çš„é¢„æœŸ
        # C++ MockAI::summarize é‡Œè§£æçš„æ˜¯ response_json["summary"]
        return {"provider": provider_name, "summary": summary_text}
    except Exception as e:
        print(f"[Error] æ€»ç»“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/{provider_name}")
async def chat_with_logs(provider_name: str, chat_request: ChatRequest):
    """
    å¤šè½®æ—¥å¿—å¯¹è¯ç«¯ç‚¹ã€‚
    æ¥æ”¶ä¸€ä¸ªåŒ…å«å†å²è®°å½•å’Œæ–°æ¶ˆæ¯çš„ JSON å¯¹è±¡ã€‚
    """
    provider = providers.get(provider_name)
    if not provider:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æˆ–æœªé…ç½® Provider '{provider_name}'ã€‚")

    try:
        # åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥æ’å…¥ä¸Šä¸‹æ–‡ç®¡ç†é€»è¾‘ï¼ˆå¦‚æ»‘åŠ¨çª—å£ã€æ‘˜è¦ç­‰ï¼‰
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬æš‚æ—¶ç›´æ¥ä¼ é€’å†å²è®°å½•
        managed_history = chat_request.history

        result = provider.chat(history=managed_history, new_message=chat_request.new_message)
        
        return {"provider": provider_name, "response": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¹è¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    

if __name__ == "__main__":
    # host="0.0.0.0" å…è®¸å±€åŸŸç½‘è®¿é—®ï¼Œ"127.0.0.1" ä»…é™æœ¬æœº
    # workers=1 å•è¿›ç¨‹æ¨¡å¼ï¼Œé€‚åˆè°ƒè¯•
    print(f"ğŸš€ LogSentinel AI Proxy æ­£åœ¨ç«¯å£ 8001 ä¸Šå¯åŠ¨...")
    uvicorn.run(app, host="127.0.0.1", port=8001)
