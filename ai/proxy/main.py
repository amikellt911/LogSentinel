# ai/proxy/main.py
import os
from fastapi import FastAPI, Request, HTTPException
from dotenv import load_dotenv
from typing import List, Dict, Any
from pydantic import BaseModel
from pathlib import Path
import uvicorn
import sys
# ==========================================
# 1. è·¯å¾„ä¸ç¯å¢ƒé…ç½® (æœ€å…ˆæ‰§è¡Œ)
# ==========================================

# è·å–å½“å‰æ–‡ä»¶ (main.py) çš„ç»å¯¹è·¯å¾„
# e.g., /home/llt/.../LogSentinel/ai/proxy/main.py
current_file = Path(__file__).resolve()

# è·å–é¡¹ç›®æ ¹ç›®å½• (LogSentinel/)
# main.py -> proxy -> ai -> LogSentinel
project_root = current_file.parent.parent.parent

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ Python æœç´¢è·¯å¾„ï¼Œè§£å†³ "ImportError"
sys.path.append(str(project_root))

# åŠ è½½ .env æ–‡ä»¶
# å‡è®¾ .env åœ¨ ai/ ç›®å½•ä¸‹ (main.py -> proxy -> ai)
dotenv_path = current_file.parent.parent / '.env'
print(f"Attempting to load .env file from: {dotenv_path}")
load_dotenv(dotenv_path=dotenv_path)

# ==========================================
# 2. å¯¼å…¥æ¨¡å— (å¿…é¡»åœ¨ sys.path è®¾ç½®ä¹‹å)
# ==========================================
# ç°åœ¨å¯ä»¥ä½¿ç”¨ç»å¯¹å¯¼å…¥äº†
from ai.proxy.providers.base import AIProvider
from ai.proxy.providers.gemini import GeminiProvider
from ai.proxy.providers.mock import MockProvider
from ai.proxy.schemas import BatchRequestSchema,ChatRequest,SummarizeRequest,BATCH_PROMPT_TEMPLATE,SUMMARIZE_PROMPT_TEMPLATE


# --- åº”ç”¨è®¾ç½® ---
app = FastAPI(
    title="LogSentinel AI Proxy",
    description="ä¸€ä¸ªç”¨äºä»£ç†ä¸åŒAIæä¾›å•†æœåŠ¡çš„ä¸­é—´å±‚ã€‚",
    version="1.0.0",
)

# --- Provider å®ä¾‹åŒ–å’Œæ³¨å†Œ ---
# åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºæ‰€æœ‰å¯ç”¨çš„'è½¬æ¢æ’å¤´'å®ä¾‹ï¼Œå¹¶æ”¾å…¥ä¸€ä¸ªå­—å…¸ä¸­è¿›è¡Œç®¡ç†ã€‚
# è¿™ç§æ–¹å¼ä½¿å¾—æ·»åŠ æ–°çš„Providerå˜å¾—éå¸¸å®¹æ˜“ã€‚
providers: Dict[str, AIProvider] = {}

# å°è¯•åˆå§‹åŒ–å¹¶æ³¨å†ŒGemini Provider
gemini_api_key = os.getenv("GEMINI_API_KEY")
if gemini_api_key and gemini_api_key != "YOUR_API_KEY":
    try:
        providers["gemini"] = GeminiProvider(api_key=gemini_api_key)
        print("Gemini provider initialized successfully.")
    except Exception as e:
        print(f"Failed to initialize Gemini provider: {e}")
else:
    if not gemini_api_key:
        print("GEMINI_API_KEY not found in environment variables. Gemini provider is disabled.")
    else:
        print("Found placeholder GEMINI_API_KEY. Please replace 'YOUR_API_KEY' in ai/.env. Gemini provider is disabled.")
providers["mock"] = MockProvider(delay=0.5)
# æœªæ¥å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å¹¶æ³¨å†ŒOpenAI, Claudeç­‰å…¶ä»–Provider
# openai_api_key = os.getenv("OPENAI_API_KEY")
# if openai_api_key:
#     providers["openai"] = OpenAIProvider(api_key=openai_api_key)


# --- API ç«¯ç‚¹å®šä¹‰ ---

@app.get("/")
def read_root():
    return {"status": "LogSentinel AI Proxy is running", "available_providers": list(providers.keys())}

@app.post("/analyze/{provider_name}")
async def analyze_log(provider_name: str, request: Request):
    """
    å•æ¬¡æ—¥å¿—åˆ†æç«¯ç‚¹ã€‚
    æ¥æ”¶çº¯æ–‡æœ¬æ ¼å¼çš„æ—¥å¿—ã€‚
    """
    provider = providers.get(provider_name)
    if not provider:
        raise HTTPException(status_code=404, detail=f"Provider '{provider_name}' not found or not configured.")

    try:
        log_text = (await request.body()).decode('utf-8')
        # ä¼˜åŒ–çš„ Promptï¼šæ˜ç¡®åˆ†æè¦æ±‚å’Œé£é™©ç­‰çº§åˆ¤æ–­æ ‡å‡†
        default_prompt = """You are a professional software engineer and log analysis expert. Analyze the following log entry and provide:

1. A concise summary of the error or issue
2. Risk level assessment:
   - 'high': System crashes, data loss, security vulnerabilities, critical service failures
   - 'medium': Performance degradation, non-critical errors, warnings that may escalate
   - 'low': Informational messages, minor warnings, expected errors
   - 'info': Normal operational logs, state changes, heartbeats
   - 'unknown': Unintelligible logs, binary data, or insufficient context to determine risk
3. Root cause analysis based on the log content
4. Actionable solution or remediation steps

Provide your analysis in a structured format."""
        
        result = provider.analyze(log_text=log_text, prompt=default_prompt)
        
        return {"provider": provider_name, "analysis": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred during analysis: {e}")

@app.post("/analyze/batch/{provider_name}")
async def analyze_log_batch(provider_name: str, request: BatchRequestSchema):
    provider=providers.get(provider_name)
    if not provider:
        raise HTTPException(status_code=404, detail=f"Provider '{provider_name}' not found or not configured.")
    try:
        logs_list=[item.model_dump() for item in request.batch]
        results=provider.analyze_batch(logs_list,BATCH_PROMPT_TEMPLATE)
        return {"provider": provider_name, "results": results}
    except Exception as e:
        print(f"[Error] Batch analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/summarize/{provider_name}")
async def summarize_logs(provider_name: str, request_data: SummarizeRequest):
    """
    Reduce é˜¶æ®µï¼šæ¥æ”¶ä¸€æ‰¹ LogAnalysisResultï¼Œç”Ÿæˆå…¨å±€æ€»ç»“ã€‚
    """
    provider = providers.get(provider_name)
    if not provider:
        raise HTTPException(status_code=404, detail=f"Provider '{provider_name}' not found")
    try:
        # å°† Pydantic å¯¹è±¡åˆ—è¡¨è½¬ä¸º Dict åˆ—è¡¨
        results_list = [item.model_dump() for item in request_data.results]
        # è°ƒç”¨ Provider çš„ summarize æ¥å£
        summary_text = provider.summarize(results_list, prompt=SUMMARIZE_PROMPT_TEMPLATE)
        # è¿”å›æ ¼å¼è¦åŒ¹é… C++ ç«¯çš„ expectations
        # C++ MockAI::summarize é‡Œè§£æçš„æ˜¯ response_json["summary"]
        return {"provider": provider_name, "summary": summary_text}
    except Exception as e:
        print(f"[Error] Summarize failed: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/{provider_name}")
async def chat_with_logs(provider_name: str, chat_request: ChatRequest):
    """
    å¤šè½®æ—¥å¿—å¯¹è¯ç«¯ç‚¹ã€‚
    æ¥æ”¶ä¸€ä¸ªåŒ…å«å†å²è®°å½•å’Œæ–°æ¶ˆæ¯çš„JSONå¯¹è±¡ã€‚
    """
    provider = providers.get(provider_name)
    if not provider:
        raise HTTPException(status_code=404, detail=f"Provider '{provider_name}' not found or not configured.")

    try:
        # åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥æ’å…¥ä¸Šä¸‹æ–‡ç®¡ç†é€»è¾‘ï¼ˆå¦‚æ»‘åŠ¨çª—å£ã€æ‘˜è¦ç­‰ï¼‰
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬æš‚æ—¶ç›´æ¥ä¼ é€’å†å²è®°å½•
        managed_history = chat_request.history

        result = provider.chat(history=managed_history, new_message=chat_request.new_message)
        
        return {"provider": provider_name, "response": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred during chat: {e}")
    

if __name__ == "__main__":
    # host="0.0.0.0" å…è®¸å±€åŸŸç½‘è®¿é—®ï¼Œ"127.0.0.1" ä»…é™æœ¬æœº
    # workers=1 å•è¿›ç¨‹æ¨¡å¼ï¼Œé€‚åˆè°ƒè¯•
    print(f"ğŸš€ Starting LogSentinel AI Proxy on port 8001...")
    uvicorn.run(app, host="127.0.0.1", port=8001)